{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing ../bench/ann-thyroid/ann-thyroid.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/ann-thyroid/ann-thyroid_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/ann-thyroid/ann-thyroid_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/appendicitis/appendicitis.csv...\n",
      "considering 106 instances\n",
      "considering 106 instances\n",
      "loading model from  ../models/appendicitis/appendicitis_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/appendicitis/appendicitis_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/biodegradation/biodegradation.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/biodegradation/biodegradation_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/biodegradation/biodegradation_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/divorce/divorce.csv...\n",
      "considering 150 instances\n",
      "considering 150 instances\n",
      "loading model from  ../models/divorce/divorce_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/divorce/divorce_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/ecoli/ecoli.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/ecoli/ecoli_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/ecoli/ecoli_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/glass2/glass2.csv...\n",
      "considering 162 instances\n",
      "considering 162 instances\n",
      "loading model from  ../models/glass2/glass2_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/glass2/glass2_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/ionosphere/ionosphere.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/ionosphere/ionosphere_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/ionosphere/ionosphere_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/pendigits/pendigits.csv...\n",
      "considering 200 instances\n",
      "considering 110 instances\n",
      "loading model from  ../models/pendigits/pendigits_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/pendigits/pendigits_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/promoters/promoters.csv...\n",
      "considering 106 instances\n",
      "considering 106 instances\n",
      "loading model from  ../models/promoters/promoters_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/promoters/promoters_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/segmentation/segmentation.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/segmentation/segmentation_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/segmentation/segmentation_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/shuttle/shuttle.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/shuttle/shuttle_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/shuttle/shuttle_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/sonar/sonar.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/sonar/sonar_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/sonar/sonar_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/spambase/spambase.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/spambase/spambase_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/spambase/spambase_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/texture/texture.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/texture/texture_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/texture/texture_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/threeOf9/threeOf9.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/threeOf9/threeOf9_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/threeOf9/threeOf9_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/twonorm/twonorm.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/twonorm/twonorm_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/twonorm/twonorm_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/vowel/vowel.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/vowel/vowel_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/vowel/vowel_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/wdbc/wdbc.csv...\n",
      "considering 200 instances\n",
      "considering 200 instances\n",
      "loading model from  ../models/wdbc/wdbc_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/wdbc/wdbc_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/wine-recognition/wine-recognition.csv...\n",
      "considering 178 instances\n",
      "considering 178 instances\n",
      "loading model from  ../models/wine-recognition/wine-recognition_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/wine-recognition/wine-recognition_nbestim_50_maxdepth_3_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/wpbc/wpbc.csv...\n",
      "considering 194 instances\n",
      "considering 194 instances\n",
      "loading model from  ../models/wpbc/wpbc_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/wpbc/wpbc_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n",
      "processing ../bench/zoo/zoo.csv...\n",
      "considering 59 instances\n",
      "considering 59 instances\n",
      "loading model from  ../models/zoo/zoo_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl\n",
      "loading data from  ../models/zoo/zoo_nbestim_50_maxdepth_4_testsplit_0.2.mod.pkl.splitdata.pkl\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from anchor_wrap import anchor_call\n",
    "\n",
    "import math\n",
    "from options import Options\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from xgbooster import XGBooster\n",
    "import resource\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num = 50\n",
    "verbose = 1\n",
    "\n",
    "with open(\"datasets.list\", 'r') as fp:\n",
    "    datasets = [tuple(line.strip().split(' ')) for line in fp.readlines() if line]\n",
    "\n",
    "\n",
    "root=\"results\"\n",
    "if not os.path.exists(root):\n",
    "    os.makedirs(root)  \n",
    "if not os.path.exists(f'{root}/anchor'):\n",
    "    os.makedirs(f'{root}/anchor')  \n",
    "\n",
    "\n",
    "# initializing the seed\n",
    "random.seed(1234)\n",
    "\n",
    "options = Options(f'./xreason.py  -q  -x \\'inst\\' somefile'.split())\n",
    "    \n",
    "\n",
    "for data, depth in datasets:\n",
    "    print(f'processing {data}...')\n",
    "\n",
    "    # reading and shuffling the instances\n",
    "    with open(data, 'r') as fp:\n",
    "        insts = [line.strip().rsplit(',', 1)[0] for line in fp.readlines()[1:]]\n",
    "        insts = list(set(insts))\n",
    "        random.shuffle(insts)\n",
    "\n",
    "        nof_insts = min(200, len(insts))\n",
    "        print(f'considering {nof_insts} instances')\n",
    "   \n",
    "        \n",
    "    base = os.path.splitext(os.path.basename(data))[0]\n",
    "    mfile = f'../models/{base}/{base}_nbestim_{num}_maxdepth_{depth}_testsplit_0.2.mod.pkl'\n",
    "    \n",
    "    ''''\n",
    "    with open(f'../logs/smt/{base}.log', 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "        insts = [line[3:].strip() for line in lines if line.startswith('i:')]\n",
    "        nof_insts = len(insts)\n",
    "        print(f'considering {nof_insts} instances')\n",
    "   '''     \n",
    "\n",
    "    log = open(f'{root}/anchor/{base}.log', 'w')\n",
    "\n",
    "    # creating booster objects\n",
    "    xgb = XGBooster(options, from_model=mfile)\n",
    "\n",
    "    atimes = []\n",
    "            \n",
    "    for i, inst in enumerate(insts):\n",
    "\n",
    "        # processing the instance\n",
    "        options.explain = [float(v.strip()) for v in inst.split(',')]\n",
    "            \n",
    "        timer = resource.getrusage(resource.RUSAGE_CHILDREN).ru_utime + \\\n",
    "                resource.getrusage(resource.RUSAGE_SELF).ru_utime            \n",
    "\n",
    "        expl1 = xgb.explain(options.explain, use_anchor=anchor_call)\n",
    "            \n",
    "        timer = resource.getrusage(resource.RUSAGE_CHILDREN).ru_utime + \\\n",
    "                resource.getrusage(resource.RUSAGE_SELF).ru_utime - timer            \n",
    "\n",
    "        print(f'i: {inst}', file=log)\n",
    "        print(f's: {len(expl1)}', file=log)\n",
    "        print(f't: {timer:.3f}', file=log)\n",
    "        print('', file=log)\n",
    "\n",
    "        atimes.append(timer)\n",
    "\n",
    "        log.flush()\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    ##################\n",
    "    print(f\"max time: {max(atimes):.2f}\", file=log)\n",
    "    print(f\"min time: {min(atimes):.2f}\", file=log)\n",
    "    print(f\"avg time: {sum(atimes)/len(atimes):.2f}\", file=log)\n",
    "    print(\"\", file=log)\n",
    "\n",
    "    #################\n",
    "    log.close()\n",
    "    log.close()\n",
    "\n",
    "    print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
